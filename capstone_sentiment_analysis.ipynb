{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import modules.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "#load dataset.\n",
    "data = pd.read_csv(\"amazon_data/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time                Summary  \\\n",
       "0                       1      5  1303862400  Good Quality Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view of labels\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 568454 reviews in this dataset.\n"
     ]
    }
   ],
   "source": [
    "#total number of reviews in dataset\n",
    "print('There are {} reviews in this dataset.'.format(data['Id'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 74258 unique products reviewed.\n",
      "The mean number of reviews for each product is 7.66.\n",
      "The median number of reviews for each product is 2.00.\n"
     ]
    }
   ],
   "source": [
    "#total/mean/median number of products reviewed in dataset\n",
    "print('There are {} unique products reviewed.'.format(data['ProductId'].nunique()))\n",
    "print('The mean number of reviews for each product is {0:.2f}.'.format(data['ProductId'].value_counts().mean()))\n",
    "print('The median number of reviews for each product is {0:.2f}.'.format(data['ProductId'].value_counts().median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 256059 unique users who contributed to the reviews.\n",
      "The mean number of reviews contributed by each user is 2.22.\n",
      "The median number of reviews contributed by each user is 1.00.\n"
     ]
    }
   ],
   "source": [
    "#total/mean/median number of users who contribtued reviews in dataset\n",
    "print('There are {} unique users who contributed to the reviews.'.format(data['UserId'].nunique()))\n",
    "print('The mean number of reviews contributed by each user is {0:.2f}.'.format(data['UserId'].value_counts().mean()))\n",
    "print('The median number of reviews contributed by each user is {0:.2f}.'.format(data['UserId'].value_counts().median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean Score is 4.18.\n",
      "The median Score is 5.00.\n",
      "There are 363122 (63%) reviews with a Score of 5.\n",
      "There are 80655 (14%) reviews with a Score of 4.\n",
      "There are 42640 (7%) reviews with a Score of 3.\n",
      "There are 29769 (5%) reviews with a Score of 2.\n",
      "There are 52268 (9%) reviews with a Score of 1.\n"
     ]
    }
   ],
   "source": [
    "#breakdown of reviews by scores\n",
    "print('The mean Score is {0:.2f}.'.format(data['Score'].mean()))\n",
    "print('The median Score is {0:.2f}.'.format(data['Score'].median()))\n",
    "print('There are {} ({}%) reviews with a Score of 5.'.format(len(data[data['Score']==5]), len(data[data['Score']==5])*100/len(data['Score'])))\n",
    "print('There are {} ({}%) reviews with a Score of 4.'.format(len(data[data['Score']==4]), len(data[data['Score']==4])*100/len(data['Score'])))\n",
    "print('There are {} ({}%) reviews with a Score of 3.'.format(len(data[data['Score']==3]), len(data[data['Score']==3])*100/len(data['Score'])))\n",
    "print('There are {} ({}%) reviews with a Score of 2.'.format(len(data[data['Score']==2]), len(data[data['Score']==2])*100/len(data['Score'])))\n",
    "print('There are {} ({}%) reviews with a Score of 1.'.format(len(data[data['Score']==1]), len(data[data['Score']==1])*100/len(data['Score'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264628 (46%) of all reviews have at least one mention of being helpful.\n",
      "298402 (52%) of all reviews have at least one mention of being helpful or unhelpful.\n",
      "226666 (39%) of all reviews were indicated as more helpful than unhelpful.\n",
      "50113 (8%) of all reviews were indicated as more unhelpful than helpful.\n"
     ]
    }
   ],
   "source": [
    "#breakdown of reviews by HelpfulnessNumerator and HelfulnessDenominator\n",
    "data['Unhelpful'] = data['HelpfulnessDenominator'] - data['HelpfulnessNumerator']\n",
    "\n",
    "print('{} ({}%) of all reviews have at least one mention of being helpful.'.format(np.count_nonzero(data['HelpfulnessNumerator']), 100*np.count_nonzero(data['HelpfulnessNumerator'])/np.count_nonzero(data['ProductId'])))\n",
    "print('{} ({}%) of all reviews have at least one mention of being helpful or unhelpful.'.format(np.count_nonzero(data['HelpfulnessDenominator']), 100*np.count_nonzero(data['HelpfulnessDenominator'])/np.count_nonzero(data['ProductId'])))\n",
    "print('{} ({}%) of all reviews were indicated as more helpful than unhelpful.'.format(len(data[data['HelpfulnessNumerator'] > data['Unhelpful']]),100*len(data[data['HelpfulnessNumerator'] > data['Unhelpful']])/data['Id'].count())) \n",
    "print('{} ({}%) of all reviews were indicated as more unhelpful than helpful.'.format(len(data[data['HelpfulnessNumerator'] < data['Unhelpful']]),100*len(data[data['HelpfulnessNumerator'] < data['Unhelpful']])/data['Id'].count())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 443777 (78%) positive reviews in the dataset.\n",
      "There are 124677 (21%) negative reviews in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#assuming a Score of 4 or 5 is a positive review, while a score of 1, 2 or 3 is a negative review\n",
    "good_score = len(data[data['Score']==5]) + len(data[data['Score']==4])\n",
    "bad_score = len(data[data['Score']==3]) + len(data[data['Score']==2]) + len(data[data['Score']==1])\n",
    "\n",
    "#breakdown of reviews by sentiment based on good or bad scores.\n",
    "print('There are {} ({}%) positive reviews in the dataset.'.format(good_score, good_score*100/(good_score+bad_score)))\n",
    "print('There are {} ({}%) negative reviews in the dataset.'.format(bad_score, bad_score*100/(good_score+bad_score)))\n",
    "\n",
    "#classifying scores into positive('0') and negative('1') sentiments\n",
    "data['Sentiment'] = data.Score.map({5:0, 4:0, 3:1, 2:1, 1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 397917 reviews in the train/test dataset.\n",
      "There are 170537 reviews in the hold out dataset.\n"
     ]
    }
   ],
   "source": [
    "#extract hold-out set for testing final model\n",
    "data1 = data.iloc[:397917]\n",
    "data2 = data.iloc[397917:]\n",
    "print('There are {} reviews in the train/test dataset.'.format(data1['Id'].count()))\n",
    "print('There are {} reviews in the hold out dataset.'.format(data2['Id'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 397917\n",
      "Number of samples in the training set: 278541\n",
      "Number of samples in the test set: 119376\n"
     ]
    }
   ],
   "source": [
    "#split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['Text'], data1['Sentiment'], test_size=0.3, random_state=7)\n",
    "\n",
    "print('Number of samples in the dataset: {}'.format(data1.shape[0]))\n",
    "print('Number of samples in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of samples in the test set: {}'.format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#create vectors for neural network\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(nb_words=10000)\n",
    "\n",
    "X_train_nn = tokenizer.sequences_to_matrix(X_train)\n",
    "X_test_nn = tokenizer.sequences_to_matrix(X_test)\n",
    "\n",
    "# One-hot encode the target variable\n",
    "from keras.utils import np_utils\n",
    "num_classes = 2\n",
    "y_train_nn = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test_nn = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build CountVectorizer object for NB, AdaBoost and XGBoost classifiers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "#transform and fit training data and return frequency matrix\n",
    "X_train = count_vector.fit_transform(X_train)\n",
    "\n",
    "#transform testing data and return frequency matrix\n",
    "X_test = count_vector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING NAIVE BAYES, ADABOOST AND XGBOOST CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) model in 85.0817 seconds.\n",
      "Made AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) predictions in 2.6566 seconds.\n",
      "*******\n",
      "SCORES FOR AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) CLASSIFIER\n",
      "Training Accuracy score: 0.827881712208\n",
      "Training Precision score: 0.67730054985\n",
      "Training Recall score: 0.423832245918\n",
      "Training F1 score: 0.521393630828\n",
      "-------\n",
      "Testing Accuracy score: 0.828767926551\n",
      "Testing Precision score: 0.676046961995\n",
      "Testing Recall score: 0.423832245918\n",
      "Testing F1 score: 0.522216768343\n",
      "------------------------------\n",
      "------------------------------\n",
      "Trained XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1) model in 90.1865 seconds.\n",
      "Made XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1) predictions in 4.1323 seconds.\n",
      "*******\n",
      "SCORES FOR XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1) CLASSIFIER\n",
      "Training Accuracy score: 0.823591499995\n",
      "Training Precision score: 0.851841407704\n",
      "Training Recall score: 0.245139091765\n",
      "Training F1 score: 0.380717121432\n",
      "-------\n",
      "Testing Accuracy score: 0.823691529286\n",
      "Testing Precision score: 0.843708783962\n",
      "Testing Recall score: 0.245139091765\n",
      "Testing F1 score: 0.378062114004\n",
      "------------------------------\n",
      "------------------------------\n",
      "Trained MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model in 0.1799 seconds.\n",
      "Made MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) predictions in 0.0472 seconds.\n",
      "*******\n",
      "SCORES FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) CLASSIFIER\n",
      "Training Accuracy score: 0.888063157668\n",
      "Training Precision score: 0.74328148231\n",
      "Training Recall score: 0.754584996916\n",
      "Training F1 score: 0.748890589135\n",
      "-------\n",
      "Testing Accuracy score: 0.876114126793\n",
      "Testing Precision score: 0.7164477657\n",
      "Testing Recall score: 0.754584996916\n",
      "Testing F1 score: 0.719666382333\n",
      "------------------------------\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#build classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "ada = AdaBoostClassifier()\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "#create function to fit, predict and evaluate models\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_pred_eval(clf):\n",
    "    #fit\n",
    "    train_start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_end = time()\n",
    "    print('Trained {} model in {:.4f} seconds.'.format(clf, train_end - train_start))\n",
    "    #predict\n",
    "    pred_start = time()\n",
    "    pred_test = clf.predict(X_test)\n",
    "    pred_end = time()\n",
    "    pred_train = clf.predict(X_train)\n",
    "    print('Made {} predictions in {:.4f} seconds.'.format(clf, pred_end - pred_start))\n",
    "    print('*******')\n",
    "    #evaluate\n",
    "    train_accuracy = accuracy_score(y_train, pred_train)\n",
    "    train_precision = precision_score(y_train, pred_train)\n",
    "    train_recall = recall_score(y_train, pred_train)\n",
    "    train_f1 = f1_score(y_train, pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, pred_test)\n",
    "    test_precision = precision_score(y_test, pred_test)\n",
    "    test_recall = recall_score(y_test, pred_test)\n",
    "    test_f1 = f1_score(y_test, pred_test)\n",
    "    print('SCORES FOR {} CLASSIFIER'.format(clf))\n",
    "    print('Training Accuracy score: {}'.format(train_accuracy))\n",
    "    print('Training Precision score: {}'.format(train_precision))\n",
    "    print('Training Recall score: {}'.format(train_recall))\n",
    "    print('Training F1 score: {}'.format(train_f1))\n",
    "    print('-------')\n",
    "    print('Testing Accuracy score: {}'.format(test_accuracy))\n",
    "    print('Testing Precision score: {}'.format(test_precision))\n",
    "    print('Testing Recall score: {}'.format(train_recall))\n",
    "    print('Testing F1 score: {}'.format(test_f1))\n",
    "    print('------------------------------')\n",
    "    print('------------------------------')\n",
    "\n",
    "#fit, predict and evaluate models\n",
    "#nb is the last classifier in loop\n",
    "for e in [ada, xgb, nb]:\n",
    "    train_pred_eval(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 32)            320032      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            2112        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             130         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 322,274\n",
      "Trainable params: 322,274\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build neural network model architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=X_train_nn.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile neural network model \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "278541/278541 [==============================] - 58s - loss: 0.5332 - acc: 0.7788 - precision: 0.7787 - recall: 0.7787    \n",
      "Epoch 2/3\n",
      "278541/278541 [==============================] - 56s - loss: 0.5285 - acc: 0.7788 - precision: 0.7788 - recall: 0.7788    \n",
      "Epoch 3/3\n",
      "278541/278541 [==============================] - 57s - loss: 0.5285 - acc: 0.7788 - precision: 0.7788 - recall: 0.7788    \n",
      "Trained Neural Network in 172.7064 seconds.\n",
      "119376/119376 [==============================] - 14s    \n",
      "Made Neural Network predictions in 14.2308 seconds.\n",
      "*******\n",
      "SCORES FOR NEURAL NETWORK\n",
      "Testing Accuracy score: 0.7800\n",
      "Testing Rrecision score: 0.7800\n",
      "Testing Recall score: 0.7800\n"
     ]
    }
   ],
   "source": [
    "#create function to fit and evaluate neural network \n",
    "def train_eval_nn(epoch,batch):\n",
    "    #fit model\n",
    "    train_start = time()\n",
    "    model.fit(X_train_nn, y_train_nn, nb_epoch=epoch, batch_size=batch)\n",
    "    train_end = time()\n",
    "    print('Trained Neural Network in {:.4f} seconds.'.format(train_end - train_start))\n",
    "    #evaluate model\n",
    "    pred_start = time()\n",
    "    scores_nn = model.evaluate(X_test_nn, y_test_nn)\n",
    "    pred_end = time()\n",
    "    print('Made Neural Network predictions in {:.4f} seconds.'.format(pred_end - pred_start))\n",
    "    print('*******')\n",
    "    print('SCORES FOR NEURAL NETWORK')\n",
    "    print('Testing Accuracy score: {0:.4f}'.format(scores_nn[1]))\n",
    "    print('Testing Rrecision score: {0:.4f}'.format(scores_nn[2]))\n",
    "    print('Testing Recall score: {0:.4f}'.format(scores_nn[3]))\n",
    "\n",
    "#train and evaluate neural network\n",
    "train_eval_nn(3,32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFINEMENT(I) - INCREASE FEATURES BY COMBINING 'SUMMARY' AND 'TEXT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#create new column with combined text from 'Summary' and 'Text'\n",
    "data1['Summary'] = data1['Summary'].astype(str)\n",
    "data1['text_all'] = data1['Summary'] + \" \" + data1['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['text_all'], data1['Sentiment'], test_size=0.3, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform and fit training data and return frequency matrix\n",
    "X_train = count_vector.fit_transform(X_train)\n",
    "\n",
    "#transform testing data and return frequency matrix\n",
    "X_test = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model in 0.1844 seconds.\n",
      "Made MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) predictions in 0.0574 seconds.\n",
      "*******\n",
      "SCORES FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) CLASSIFIER\n",
      "Training Accuracy score: 0.901472314668\n",
      "Training Precision score: 0.764138400173\n",
      "Training Recall score: 0.802187814458\n",
      "Training F1 score: 0.782700956483\n",
      "-------\n",
      "Testing Accuracy score: 0.891577871599\n",
      "Testing Precision score: 0.742461762564\n",
      "Testing Recall score: 0.802187814458\n",
      "Testing F1 score: 0.759061039856\n",
      "------------------------------\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#implement NB classifier after refinement(I)\n",
    "train_pred_eval(nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFINEMENT(II) - ACCENTUATE FEATURES BY INCREASING 'SUMMARY' to 'TEXT' RATIO AND REPLICATING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#accentuate words in 'Summary' by creating a new column with combined text from 'Summary' and 'Text' with the ratio of 6:1\n",
    "data1['text_all'] = data1['Summary'] + \" \" + data1['Summary'] + \" \" + data1['Summary'] + \" \" + data1['Summary'] + \" \" + data1['Summary'] + \" \" + data1['Summary'] + \" \" + data1['Text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#accentuate most frequently occuring words by replicating \"text_all\"\n",
    "data1['text_all'] = data1['text_all'] + \" \" + data1['text_all'] + \" \" + data1['text_all'] + \" \" + data1['text_all'] + \" \" + data1['text_all'] + \" \" + data1['text_all'] + \" \" + data1['text_all'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['text_all'], data1['Sentiment'], test_size=0.3, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform and fit training data and return frequency matrix\n",
    "X_train = count_vector.fit_transform(X_train)\n",
    "\n",
    "#transform testing data and return frequency matrix\n",
    "X_test = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model in 0.1631 seconds.\n",
      "Made MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) predictions in 0.0500 seconds.\n",
      "*******\n",
      "SCORES FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) CLASSIFIER\n",
      "Training Accuracy score: 0.903479200549\n",
      "Training Precision score: 0.72948577319\n",
      "Training Recall score: 0.895867822248\n",
      "Training F1 score: 0.804160808852\n",
      "-------\n",
      "Testing Accuracy score: 0.890890966358\n",
      "Testing Precision score: 0.703662665436\n",
      "Testing Recall score: 0.895867822248\n",
      "Testing F1 score: 0.778294098623\n",
      "------------------------------\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#implement NB classifier after refinement(II)\n",
    "train_pred_eval(nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFINEMENT(III) - FILTER SAMPLES BY HELPFULNESSNUMERATOR AND HELPFULNESSDENOMINATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove reviews without a single HelpfulnessNumerator\n",
    "data_helpful = data1[data1['HelpfulnessNumerator'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 186054\n",
      "Number of samples in the training set: 130237\n",
      "Number of samples in the test set: 55817\n"
     ]
    }
   ],
   "source": [
    "#split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_helpful['text_all'], data_helpful['Sentiment'], test_size=0.3, random_state=7)\n",
    "\n",
    "print('Number of samples in the dataset: {}'.format(data_helpful.shape[0]))\n",
    "print('Number of samples in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of samples in the test set: {}'.format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform and fit training data and return frequency matrix\n",
    "X_train = count_vector.fit_transform(X_train)\n",
    "\n",
    "#transform testing data and return frequency matrix\n",
    "X_test = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model in 0.0848 seconds.\n",
      "Made MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) predictions in 0.0263 seconds.\n",
      "*******\n",
      "SCORES FOR MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) CLASSIFIER\n",
      "Training Accuracy score: 0.915377350523\n",
      "Training Precision score: 0.788936011309\n",
      "Training Recall score: 0.904913418589\n",
      "Training F1 score: 0.842954244268\n",
      "-------\n",
      "Testing Accuracy score: 0.894404930398\n",
      "Testing Precision score: 0.750171671141\n",
      "Testing Recall score: 0.904913418589\n",
      "Testing F1 score: 0.803060678963\n",
      "------------------------------\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#implement NB classifier after refinement(III)\n",
    "train_pred_eval(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING FINAL MODEL WITH HOLD OUT SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made hold out predictions in 0.0652 seconds.\n",
      "*******\n",
      "SCORES FOR HOLD OUT DATASET\n",
      "Testing Accuracy score: 0.856705582953\n",
      "Testing Precision score: 0.63839215774\n",
      "Testing Recall score: 0.775024453864\n",
      "Testing F1 score: 0.700104313677\n"
     ]
    }
   ],
   "source": [
    "#transform hold out testing data and return frequency matrix\n",
    "holdout_test = count_vector.transform(data2['Text'])\n",
    "\n",
    "#predict \n",
    "pred_start = time()\n",
    "pred_test = nb.predict(holdout_test)\n",
    "pred_end = time()\n",
    "print('Made hold out predictions in {:.4f} seconds.'.format(pred_end - pred_start))\n",
    "print('*******')\n",
    "\n",
    "#evaluate\n",
    "test_accuracy = accuracy_score(data2['Sentiment'], pred_test)\n",
    "test_precision = precision_score(data2['Sentiment'], pred_test)\n",
    "test_recall = recall_score(data2['Sentiment'], pred_test)\n",
    "test_f1 = f1_score(data2['Sentiment'], pred_test)\n",
    "print('SCORES FOR HOLD OUT DATASET')\n",
    "print('Testing Accuracy score: {}'.format(test_accuracy))\n",
    "print('Testing Precision score: {}'.format(test_precision))\n",
    "print('Testing Recall score: {}'.format(test_recall))\n",
    "print('Testing F1 score: {}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDENDUM: REFINEMENT FOR NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove unhelpful reviews from dataset\n",
    "data_helpful = data1[data1['HelpfulnessNumerator'] >= data1['Unhelpful']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 362853\n",
      "Number of samples in the training set: 253997\n",
      "Number of samples in the test set: 108856\n"
     ]
    }
   ],
   "source": [
    "#split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_helpful['text_all'], data_helpful['Sentiment'], test_size=0.3, random_state=7)\n",
    "\n",
    "print('Number of samples in the dataset: {}'.format(data_helpful.shape[0]))\n",
    "print('Number of samples in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of samples in the test set: {}'.format(X_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create vectors for neural network\n",
    "X_train_nn = tokenizer.sequences_to_matrix(X_train)\n",
    "X_test_nn = tokenizer.sequences_to_matrix(X_test)\n",
    "\n",
    "# One-hot encoding the output\n",
    "y_train_nn = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test_nn = np_utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "253997/253997 [==============================] - 51s - loss: 0.4789 - acc: 0.8151 - precision: 0.8151 - recall: 0.8151    \n",
      "Epoch 2/3\n",
      "253997/253997 [==============================] - 52s - loss: 0.4788 - acc: 0.8151 - precision: 0.8151 - recall: 0.8151    \n",
      "Epoch 3/3\n",
      "253997/253997 [==============================] - 52s - loss: 0.4788 - acc: 0.8151 - precision: 0.8151 - recall: 0.8151    \n",
      "Trained Neural Network in 155.9739 seconds.\n",
      "108832/108856 [============================>.] - ETA: 0sMade Neural Network predictions in 12.7906 seconds.\n",
      "*******\n",
      "SCORES FOR NEURAL NETWORK\n",
      "Testing Accuracy score: 0.8161\n",
      "Testing Rrecision score: 0.8161\n",
      "Testing Recall score: 0.8161\n"
     ]
    }
   ],
   "source": [
    "#fit and evaluate neural network after refinement\n",
    "train_eval_nn(3,32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING NEURAL NETWORK WITH HOLDOUT SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create vectors for neural network\n",
    "holdout_test_X_nn = tokenizer.sequences_to_matrix(data2['Text'])\n",
    "\n",
    "# One-hot encode the target variable\n",
    "num_classes = 2\n",
    "holdout_test_y_nn = np_utils.to_categorical(data2['Sentiment'], num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170368/170537 [============================>.] - ETA: 0sMade Neural Network predictions in 19.8762 seconds.\n",
      "*******\n",
      "SCORES FOR NEURAL NETWORK\n",
      "Testing Accuracy score: 0.7842\n",
      "Testing Rrecision score: 0.7842\n",
      "Testing Recall score: 0.7842\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "pred_start = time()\n",
    "scores_nn = model.evaluate(holdout_test_X_nn, holdout_test_y_nn)\n",
    "pred_end = time()\n",
    "print('Made Neural Network predictions in {:.4f} seconds.'.format(pred_end - pred_start))\n",
    "print('*******')\n",
    "print('SCORES FOR NEURAL NETWORK')\n",
    "print('Testing Accuracy score: {0:.4f}'.format(scores_nn[1]))\n",
    "print('Testing Rrecision score: {0:.4f}'.format(scores_nn[2]))\n",
    "print('Testing Recall score: {0:.4f}'.format(scores_nn[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
